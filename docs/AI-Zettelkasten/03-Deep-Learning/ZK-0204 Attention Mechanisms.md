---
title: Attention Mechanisms
id: ZK-0204
tags: [deep-learning, attention]
created: 2025-11-13
updated:
---

**Definition**
- Attention allocates computational focus to the most relevant parts of an input sequence or feature map.

**Key Ideas**
- Variants include additive, multiplicative, self-attention, and cross-attention.
- Improves interpretability by exposing weight distributions across tokens.
- Efficient approximations (Performer, FlashAttention) mitigate quadratic cost.



**Real-World Use Cases**
- Highlight relevant clauses in bilingual contracts for auditors.
- Improve medical image triage by focusing on lesions.

**Technologies & Tooling**
- Attention visualization toolkits like BertViz.
- Efficient attention libs (xFormers, Performer).

**Related Notes**
- [[ZK-0203 Transformers]]
- [[ZK-0304 Multimodality]]
- [[ZK-0903 Long Context Models]]

**Further Expansion**
- Summarize linear-attention techniques for edge inference
- Create visualization recipe for attention maps in policy briefings
